<!doctypehtml><html prefix="og: http://ogp.me/ns#"lang=en_US><meta charset=utf-8><meta content=width=device-width,initial-scale=1 name=viewport><title>I sped up serde_json strings by 20% | purplesyringa's blog</title><link href=../../all.css rel=stylesheet><link href=../../blog.css rel=stylesheet><link href=../../vendor/Temml-Local.css rel=stylesheet><link crossorigin href=https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,100..900;1,100..900&family=Roboto+Mono:ital,wght@0,100..700;1,100..700&family=Roboto:ital,wght@0,400;0,700;1,400;1,700&family=Slabo+27px&display=swap rel=stylesheet><link media="screen and (prefers-color-scheme: dark"href=../../vendor/atom-one-dark.min.css rel=stylesheet><link media="screen and (prefers-color-scheme: light"href=../../vendor/atelier-cave-light.min.css rel=stylesheet><link title="Blog posts"href=../../blog/feed.rss rel=alternate type=application/rss+xml><meta content="I sped up serde_json strings by 20%"property=og:title><meta content=article property=og:type><meta content=https://purplesyringa.moe/blog/i-sped-up-serde-json-strings-by-20-percent/og.png property=og:image><meta content=https://purplesyringa.moe/blog/i-sped-up-serde-json-strings-by-20-percent/ property=og:url><meta content="I have recently done some performance work and realized that reading about my experience could be entertaining. Teaching to think is just as important as teaching to code, but this is seldom done; I think something I‚Äôve done last month is a great opportunity to draw the curtain a bit.
serde is the Rust framework for serialization and deserialization. Everyone uses it, and it‚Äôs the default among the ecosystem. serde_json is the official serde ‚Äúmixin‚Äù for JSON, so when people need to parse stuff, that‚Äôs what they use instinctively. There are other libraries for JSON parsing, like simd-json, but serde_json is overwhelmingly used: it has 26916 dependents at the time of this post, compared to only 66 for simd-json.
This makes serde_json a good target (not in a Jia Tan way) for optimization. Chances are, many of those 26916 users would profit from switching to simd-json, but as long as they aren‚Äôt doing that, smaller optimizations are better than nothing, and such improvements are reapt across the ecosystem"property=og:description><meta content=en_US property=og:locale><meta content="purplesyringa's blog"property=og:site_name><meta content=summary_large_image name=twitter:card><meta content=https://purplesyringa.moe/blog/i-sped-up-serde-json-strings-by-20-percent/og.png name=twitter:image><body><header><div class=viewport-container><div class=media><a href=https://github.com/purplesyringa><img src=../../images/github-mark-white.svg></a></div><h1><a href=/>purplesyringa</a></h1><nav><a href=../..>about</a><a class=current href=../../blog/>blog</a><a href=../../sink/>kitchen sink</a></nav></div></header><section><div class=viewport-container><h2>I sped up serde_json strings by 20%</h2><time>August 20, 2024</time><p>I have recently done some performance work and realized that reading about my experience could be entertaining. Teaching to <em>think</em> is just as important as teaching to <em>code</em>, but this is seldom done; I think something I‚Äôve done last month is a great opportunity to draw the curtain a bit.<p><code>serde</code> is <em>the</em> Rust framework for serialization and deserialization. Everyone uses it, and it‚Äôs the default among the ecosystem. <code>serde_json</code> is the official <code>serde</code> ‚Äúmixin‚Äù for JSON, so when people need to parse stuff, that‚Äôs what they use instinctively. There are other libraries for JSON parsing, like <a href=https://lib.rs/crates/simd-json>simd-json</a>, but <code>serde_json</code> is overwhelmingly used: it has <a href=https://crates.io/crates/serde_json/reverse_dependencies>26916</a> dependents at the time of this post, compared to only <a href=https://crates.io/crates/simd-json/reverse_dependencies>66</a> for <code>simd-json</code>.<p>This makes <code>serde_json</code> a good target <s>(not in a Jia Tan way)</s> for optimization. Chances are, many of those 26916 users would profit from switching to <code>simd-json</code>, but as long as they aren‚Äôt doing that, smaller optimizations are better than nothing, and such improvements are reapt across the ecosystem.<p class=next-group><span class=side-header><span>Where do I start?</span></span>I have recently been working on the <a href=../you-might-want-to-use-panics-for-error-handling/>#[iex]</a> library. I used <code>serde</code> and <code>serde_json</code> as benchmarks and noticed some questionable decisions in their performance-critical code while rewriting it to better suit <code>#[iex]</code>.<p><code>#[iex]</code> focuses on error handling, so the error path is the first thing I benchmarked. To my surprise, <code>serde_json</code>‚Äôs error path was more than 2x slower than the success path on the same data:<div class=table-wrapper><table><thead><tr><td rowspan=2>Speed (MB/s, higher is better)<th colspan=2><code>canada</code><th colspan=2><code>citm_catalog</code><th colspan=2><code>twitter</code><tr><th>DOM<th>struct<th>DOM<th>struct<th>DOM<th>struct<tbody><tr><td>Success path<td align=center>283<td align=center>416<td align=center>429<td align=center>864<td align=center>275<td align=center>541<tr><td>Error path<td align=center>122<td align=center>168<td align=center>135<td align=center>195<td align=center>142<td align=center>226<tr><td>Slowdown<td align=center>-57%<td align=center>-60%<td align=center>-69%<td align=center>-77%<td align=center>-48%<td align=center>-58%</table></div><p>Why? Error propagation cannot be that slow. Profiling via <code>perf</code> reveals that the bottleneck is this innocent function:<pre><code class=language-rust><span class=hljs-keyword>fn</span> <span class="hljs-title function_">position_of_index</span>(&<span class=hljs-keyword>self</span>, i: <span class=hljs-type>usize</span>) <span class=hljs-punctuation>-></span> Position {
    <span class=hljs-keyword>let</span> <span class=hljs-keyword>mut </span><span class=hljs-variable>position</span> = Position { line: <span class=hljs-number>1</span>, column: <span class=hljs-number>0</span> };
    <span class=hljs-keyword>for</span> <span class=hljs-variable>ch</span> <span class=hljs-keyword>in</span> &<span class=hljs-keyword>self</span>.slice[..i] {
        <span class=hljs-keyword>match</span> *ch {
            <span class=hljs-string>b'\n'</span> => {
                position.line += <span class=hljs-number>1</span>;
                position.column = <span class=hljs-number>0</span>;
            }
            _ => {
                position.column += <span class=hljs-number>1</span>;
            }
        }
    }
    position
}
</code></pre><p>‚Ä¶which is called from <code>position()</code> to format the error, which is documented as:<pre><code class=language-rust><span class=hljs-comment>/// Position of the most recent call to next().</span>
<span class=hljs-comment>///</span>
<span class=hljs-comment>/// ...</span>
<span class=hljs-comment>///</span>
<span class=hljs-comment>/// Only called in case of an error, so performance is not important.</span>
</code></pre><p>‚Ä¶Well. I agree that between a faster success path and a faster error path, the former wins, but taking more time than just parsing just to format an error is taking it way too far.<p>Can we do anything about this? <code>position_of_index()</code> wants to convert an index in a string to a line/column pair. To do that, we can reduce the problem to two simpler ones:<ul><li>Count <code>\n</code>s in <code>self.slice[..i]</code>; that‚Äôs going to be the 0-based line number, and<li>Find the last <code>\n</code> in <code>self.slice[..i]</code> and subtract its position from <code>i</code>; that‚Äôs going to be the 0-based column number.</ul><p>Searching a string for a single-character needle is a long-solved problem. In C, we use <code>strchr</code> for that; in Rust, we use the <a href=https://crates.io/crates/memchr>memchr</a> crate. In fact, this crate also provides <a href=https://docs.rs/memchr/2.7.4/src/memchr/memchr.rs.html#327-333>an optimized way</a> to <em>count</em> occurences, which we need for the first subproblem.<p><code>memchr</code> uses SIMD in both cases, so it‚Äôs a lot faster than a naive loop. Indeed, replacing the implementation above with:<pre><code class=language-rust><span class=hljs-keyword>fn</span> <span class="hljs-title function_">position_of_index</span>(&<span class=hljs-keyword>self</span>, i: <span class=hljs-type>usize</span>) <span class=hljs-punctuation>-></span> Position {
    <span class=hljs-keyword>let</span> <span class=hljs-variable>start_of_line</span> = <span class=hljs-keyword>match</span> memchr::<span class="hljs-title function_ invoke__">memrchr</span>(<span class=hljs-string>b'\n'</span>, &<span class=hljs-keyword>self</span>.slice[..i]) {
        <span class="hljs-title function_ invoke__">Some</span>(position) => position + <span class=hljs-number>1</span>,
        <span class=hljs-literal>None</span> => <span class=hljs-number>0</span>,
    };
    Position {
        line: <span class=hljs-number>1</span> + memchr::<span class="hljs-title function_ invoke__">memchr_iter</span>(<span class=hljs-string>b'\n'</span>, &<span class=hljs-keyword>self</span>.slice[..start_of_line]).<span class="hljs-title function_ invoke__">count</span>(),
        column: i - start_of_line,
    }
}
</code></pre><p>‚Ä¶results in a great improvement:<div class=table-wrapper><table><thead><tr><td rowspan=2>Speed (MB/s, higher is better)<th colspan=2><code>canada</code><th colspan=2><code>citm_catalog</code><th colspan=2><code>twitter</code><tr><th>DOM<th>struct<th>DOM<th>struct<th>DOM<th>struct<tbody><tr><td>Success path<td align=center>283<td align=center>416<td align=center>429<td align=center>864<td align=center>275<td align=center>541<tr><td>Error path (<code>memchr</code>)<td align=center>216<td align=center>376<td align=center>238<td align=center>736<td align=center>210<td align=center>492<tr><td>Slowdown<td align=center>-24%<td align=center>-10%<td align=center>-45%<td align=center>-15%<td align=center>-24%<td align=center>-9%</table></div><p>The error path is still slower than the success path, but the difference is a lot less prominent now.<p>I submitted <a href=https://github.com/serde-rs/json/pull/1160>a PR introducing this optimization</a> and wondered if it‚Äôs going to be merged. After all, <code>serde_json</code> has very few dependencies, and dtolnay seems to focus on build times, so would a PR adding a new dependency make it?<p>To my shock, the PR was quickly merged! Not a bad first contribution.<p class=next-group><span class=side-header><span>What next?</span></span>dtolnay advised me to look for other places where a similar optimization could be applied, so that‚Äôs what I did. (I can‚Äôt overestimate how helpful he was to me during this endeavor.)<p>The first place I found is this loop in string parsing:<pre><code class=language-rust><span class=hljs-keyword>while</span> <span class=hljs-keyword>self</span>.index < <span class=hljs-keyword>self</span>.slice.<span class="hljs-title function_ invoke__">len</span>() && !ESCAPE[<span class=hljs-keyword>self</span>.slice[<span class=hljs-keyword>self</span>.index] <span class=hljs-keyword>as</span> <span class=hljs-type>usize</span>] {
    <span class=hljs-keyword>self</span>.index += <span class=hljs-number>1</span>;
}
</code></pre><p>What we want here is to find the first non-escape character. ‚ÄúEscape‚Äù characters are <code>\\</code> (for obvious reasons) and <code>"</code> (because it marks the end of the string), but also all ASCII codes up to 0x1f, because the JSON specification <a href=https://www.crockford.com/mckeeman.html>forbids</a> control codes in strings (so e.g. <code>"line 1\nline 2"</code> is valid JSON, but replacing the <code>\n</code> with a literal newline invalidates it).<p><em>If</em> all I needed was to find the first <code>\\</code> or <code>"</code>, the <a href=https://docs.rs/memchr/latest/memchr/fn.memchr2.html>memchr2</a> function provided by <code>memchr</code> would suffice. But I need something more complicated, so how am I supposed to go about it?<h2>Looking for escape</h2><p class=next-group><span class=side-header><span>First try</span></span>The first idea dtolnay and I had wasn‚Äôt a good one, and it touches a tangential topic, but I still think it‚Äôs important to discuss to learn how to not make the same mistake.<p>The idea was:<ul><li>Use <code>memchr2</code> to find the first <code>\</code> or <code>"</code>, and after that<li>Go through the string character-by-character to ensure there are no control characters.</ul><p>The idea was that offloading the search for <code>\</code> and <code>"</code> to a faster algorithm would improve the performance overall.<p>In reality, this turned out to be <em>slower</em> than the original code, because looping over the string <em>twice</em>, quickly and then slowly, is always bound to be worse than looping over the string <em>once</em>, just as slowly. Sure, a byte comparison (<code>ch < 0x20</code>) is a little bit faster than a memory access (<code>ESCAPE[...]</code>), but that effect is quickly offset by using two passes (and thus increasing memory bandwidth) instead of one.<p>It turns out that dtolnay based his intuition on <a href=https://nrk.neocities.org/articles/cpu-vs-common-sense>a post</a> that studied various implementations of the standard C <code>strlcpy</code> function and found that a two-pass algorithm is faster than a single-pass algorithm. So what went wrong there?<p><code>strlcpy(char *dst, const char *src, size_t size)</code> copies a string from <code>src</code> to <code>dst</code>, truncating it to at most <code>size - 1</code> characters. The 1 byte is reserved for the always-added null terminator. The competing implementations were (adding the terminating NUL byte is irrelevant, so not listed here):<ul><li>Single-pass: perform <code>*dst++ = *src++</code> at most <code>size - 1</code> times, until <code>*src</code> is a NUL byte, and<li>Two-pass: compute <code>len = strlen(src)</code>, then call <code>memcpy(dst, src, min(len, size - 1))</code>.</ul><p>The two-pass algorithm was faster because <code>strlen</code> and <code>memcpy</code> were calls to SIMD-optimized glibc routines, but the loop of the single-pass algorithm was scalar. The author realized this and provided their own implementations of <code>strlen</code> and <code>memcpy</code>, pessimizing the two-pass <code>strlcpy</code>, so that the two algorithms were more competitive:<pre><code class=language-c><span class=hljs-type>size_t</span> <span class="hljs-title function_">bespoke_strlcpy</span><span class=hljs-params>(<span class=hljs-type>char</span> *dst, <span class=hljs-type>const</span> <span class=hljs-type>char</span> *src, <span class=hljs-type>size_t</span> size)</span> {
    <span class=hljs-type>size_t</span> len = <span class=hljs-number>0</span>;
    <span class=hljs-keyword>for</span> (; src[len] != <span class=hljs-string>'\0'</span>; ++len) {} <span class=hljs-comment>// strlen()</span>

    <span class=hljs-keyword>if</span> (size > <span class=hljs-number>0</span>) {
        <span class=hljs-type>size_t</span> to_copy = len < size ? len : size - <span class=hljs-number>1</span>;
        <span class=hljs-keyword>for</span> (<span class=hljs-type>size_t</span> i = <span class=hljs-number>0</span>; i < to_copy; ++i) <span class=hljs-comment>// memcpy()</span>
            dst[i] = src[i];
        dst[to_copy] = <span class=hljs-string>'\0'</span>;
    }
    <span class=hljs-keyword>return</span> len;
}
</code></pre><p>GCC can easily detect such loops and replace them with glibc calls, so the author also explicitly disabled this with <code>-fno-builtin</code>. Even like this, the two-pass algorithm was still faster than a single-pass one.<p>However, one detail wasn‚Äôt explicit. <code>-fno-builtin</code> does not disable <em>all</em> <code>memcpy</code>-related optimizations: the <code>memcpy</code> loop can still be vectorized, and that‚Äôs what GCC did to <code>bespoke_strlcpy</code>. So the author was actually comparing scalar <code>strlen</code> (check for NUL, loop) + vectorized <code>memcpy</code> (check for size, loop) to scalar <code>strlcpy</code> (check for NUL, check for size, loop).<p>Disabling vectorization with <code>-fno-tree-vectorize</code> makes the two-pass algorithm <em>slower</em>, as it should be, because now we‚Äôre compiling two loops (check for NUL, loop; check for size, loop) with one loop (check for NUL, check for size, loop), and the latter is faster because it puts less pressure on the branch predictor and has fewer memory accesses.<hr><p>The lesson here is that vectorization is king, but vectorizing the simpler half of the code while the leaving the complex one scalar is not going to provide any improvements. So if we want to optimize this, we have to use SIMD for both parts.<p class=next-group><span class=side-header><span>Second try</span></span>The original approach thus morphed into:<ul><li>Use <code>memchr2</code> to find the first <code>\</code> or <code>"</code>, and after that<li>Use hand-written SIMD to ensure there are no control characters.</ul><p>We would need to reinvent the bicycle, but this is quite neat if you think about it. In the success path, we <em>find positions</em> of <code>\</code> and <code>"</code>, but we only <em>check the absence</em> of control codes. So we can avoid a conditional branch in the hot loop, by replacing this:<pre><code class=language-python><span class=hljs-keyword>for</span> simd_word <span class=hljs-keyword>in</span> to_simd_words(data):
    <span class=hljs-keyword>if</span> <span class=hljs-built_in>any</span>(simd_word < <span class=hljs-number>0x20</span>):
        ...
</code></pre><p>‚Ä¶with this:<pre><code class=language-python>mask = <span class=hljs-literal>False</span>
<span class=hljs-keyword>for</span> simd_word <span class=hljs-keyword>in</span> to_simd_words(data):
    mask |= simd_word < <span class=hljs-number>0x20</span>
<span class=hljs-keyword>if</span> <span class=hljs-built_in>any</span>(mask):
    ...
</code></pre><p>However, we quickly realized that this was a losing battle. We would slow down short strings by invoking a (runtime-selected!) function to search for <code>\</code> and <code>"</code>, but we would pessimize longer strings by reading memory twice <em>too</em>. We needed something better.<p class=next-group><span class=side-header><span>Accepting fate</span></span>We really needed to search for <code>\</code>, <code>"</code> <em>and</em> control codes in one pass.<p>But I tried hard to keep <code>serde_json</code> as simple as it was. I don‚Äôt usually care about code complexity in my projects, but something another person has to maintain should preferably be as uninvasive as possible. This made separately implementing SIMD for different platforms a no-go.<p class=next-group><span class=side-header><span>Mycroft‚Äôs trick</span></span>However, there is one technique people used before common processors supported SIMD natively. Instead of processing 128-bit words, we could process 64-bit words, using bitwise operations to simulate per-element behavior. This idea is called SIMD Within A Register, or SWAR for short. To give a simple example, converting 8 Latin letters stored in a 64-bit word to lowercase is as easy as computing <code>x | 0x2020202020202020</code>.<p>What I wanted to do is search for a control character in an 64-bit word, implicitly split into 8 bytes.<p>The way this works is that for <code>c: i8</code>, the condition we‚Äôre looking for is <code>c >= 0 && c < 0x20</code>, which can be rewritten as <code>c >= 0 && c - 0x20 < 0</code>. This just checks that the sign bit of <code>c</code> is <code>0</code> and the sign bit of <code>c - 0x20</code> is <code>1</code>, which is equivalent to <code>!c & (c - 0x20) & 0x80 != 0</code>.<p>So for 8 packed bytes, we compute <code>!c & (c - 0x2020202020202020) & 0x8080808080808080</code>. If it‚Äôs <code>0</code>, great, no control character. If it‚Äôs non-zero, we find the least significant non-zero byte in the mask, and that‚Äôs our first occurence of a control character.<p>There‚Äôs just one problem. The <code>c - 0x20</code> in <code>c >= 0 && c - 0x20 < 0</code> is a wrapping subtraction, but performing a 64-bit subtraction can propagate carry/borrow between bytes. This is, however, not a problem: the borrow can only be propagated from a byte if it‚Äôs less than <code>0x20</code>, and only to more significant bytes. We only wish to find the least significant control byte, so we don‚Äôt care if it corrupts more significant bytes.<p>This, of course, only works on little-endian machines. For big-endian machines, <code>c</code> has to be bytereversed.<p>What about matching <code>\</code> (and <code>"</code>) though? The condition for <code>\</code> is as simple as <code>c ^ '\\' >= 0 && c ^ '\\' < 1</code>; this is just the formula above with <code>0x20</code> replaced with <code>0x01</code>. <a href=https://github.com/serde-rs/json/pull/1161#discussion_r1713040513>The cherry on top</a> is that <code>'\\'</code> doesn‚Äôt have the sign bit set, so <code>c ^ '\\' >= 0</code> is equivalent to <code>c >= 0</code>.<p>All in all, the formula simplifies to:<pre><code class=language-rust>!c
& (
    (c - <span class=hljs-number>0x2020202020202020</span>)
    | ((c ^ <span class=hljs-string>'\\'</span>) - <span class=hljs-number>0x0101010101010101</span>)
    | ((c ^ <span class=hljs-string>'"'</span>) - <span class=hljs-number>0x0101010101010101</span>)
)
& <span class=hljs-number>0x8080808080808080</span>
</code></pre><p>This is <code>9</code> bitwise operations (counting <code>a & !b</code> as one instead of two). For comparison, this would require <code>7</code> SIMD operations on x86, so that‚Äôs quite close to what we‚Äôd get from SIMD, just with 2x or 4x smaller throughput, depending on whether AVX is available.<p>But for short strings, throughput doesn‚Äôt matter. Latency does. Maybe I made a mistake while trying out different variations, but this SWAR code was more efficient than ‚Äúreal‚Äù SIMD code on <a href=https://github.com/serde-rs/json-benchmark>json-benchmark</a>, probably because of this effect. Whatever the reason, this is the code we settled on eventually.<p class=next-group><span class=side-header><span>The other end</span></span>Whenever you optimize something by unrolling a loop or using SIMD, a good question to ponder is whether the array is long enough to profit from this. For example, using SIMD to find the length of a 0-16 byte string in a branchless way is neat, but can easily lose to the simplest <code>strlen</code> implementation if the strings are usually just 3 bytes long.<p>Something similar happened here. For strings of around 5 characters, the SWAR approach became slower than scalar code. We decided that regressing such very short strings is a worthwhile investment if we get faster code in other cases.<p>However, there is one very common short string ‚Äì the empty string <code>""</code>. Also, due to a technicality, a similar regression applied to strings with consecutive escapes, e.g. <code>\r\n</code> or <code>\uD801\uDC37</code>, which is a really common sight in Unicode-ridden data. We certainly don‚Äôt want to regress that. The fix is simple: just check if the very first character is an escape before entering the SWAR loop.<p>All in all, the improvements we got are this:<div class=table-wrapper><table><thead><tr><td rowspan=2>Speed (MB/s, higher is better)<th colspan=2><code>canada</code><th colspan=2><code>citm_catalog</code><th colspan=2><code>twitter</code><tr><th>DOM<th>struct<th>DOM<th>struct<th>DOM<th>struct<tbody><tr><td>Scalar<td align=center>291<td align=center>442<td align=center>377<td align=center>865<td align=center>305<td align=center>638<tr><td>Vectorized<td align=center>292<td align=center>442<td align=center>367<td align=center>905<td align=center>335<td align=center>785<tr><td>Speedup<td align=center>0%<td align=center>0%<td align=center>-3%<td align=center>+5%<td align=center>+10%<td align=center>+23%</table></div><p><code>citm_catalog DOM</code> is quite flickery, so in the end there aren‚Äôt even regressions on <code>json-benchmark</code>. There is one other regression though: empty strings still take a bit longer to parse, but the slowdown is luckily within 2% on a very specific microbenchmark.<h2>When lexing becomes complicated</h2><p class=next-group><span class=side-header><span>Unicode</span></span>What else about Unicode, by the way? <code>serde_json</code> can parse Unicode in both decoded and encoded formats, e.g. <code>"ü•∫"</code> and <code>"\ud83e\udd7a"</code>. While raw Unicode is trivial to parse, decoding <code>\u</code> escapes is a more complicated topic.<p>Can‚Äôt you just parse four hex digits and that‚Äôs it? Well, sort of. Number parsing is really hard, and it might surprise you how generic and complex some algorithms are.<p>Parsing a hex digit requires mapping disjoint intervals <code>'0'..='9'</code>, <code>'A'..='F'</code>, <code>'a'..='f'</code> to <code>0..16</code>. You could use conditionals for that:<pre><code class=language-rust><span class=hljs-keyword>match</span> c {
    <span class=hljs-string>b'0'</span>..=<span class=hljs-string>b'9'</span> => c - <span class=hljs-string>b'0'</span>,
    <span class=hljs-string>b'A'</span>..=<span class=hljs-string>b'F'</span> => c - <span class=hljs-string>b'A'</span> + <span class=hljs-number>10</span>,
    <span class=hljs-string>b'a'</span>..=<span class=hljs-string>b'f'</span> => c - <span class=hljs-string>b'a'</span> + <span class=hljs-number>10</span>,
    _ => <span class=hljs-keyword>return</span> <span class="hljs-title function_ invoke__">Err</span>(..),
}
</code></pre><p>‚Ä¶or branchless algorithms, which the Rust standard library does.<p>But nothing beats a LUT.<pre><code class=language-rust><span class=hljs-keyword>static</span> HEX: [<span class=hljs-type>u8</span>; <span class=hljs-number>256</span>] = {
    <span class=hljs-keyword>const</span> __: <span class=hljs-type>u8</span> = <span class=hljs-number>255</span>; <span class=hljs-comment>// not a hex digit</span>
    [
        <span class=hljs-comment>//   1   2   3   4   5   6   7   8   9   A   B   C   D   E   F</span>
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// 0</span>
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// 1</span>
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// 2</span>
        <span class=hljs-number>00</span>, <span class=hljs-number>01</span>, <span class=hljs-number>02</span>, <span class=hljs-number>03</span>, <span class=hljs-number>04</span>, <span class=hljs-number>05</span>, <span class=hljs-number>06</span>, <span class=hljs-number>07</span>, <span class=hljs-number>08</span>, <span class=hljs-number>09</span>, __, __, __, __, __, __, <span class=hljs-comment>// 3</span>
        __, <span class=hljs-number>10</span>, <span class=hljs-number>11</span>, <span class=hljs-number>12</span>, <span class=hljs-number>13</span>, <span class=hljs-number>14</span>, <span class=hljs-number>15</span>, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// 4</span>
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// 5</span>
        __, <span class=hljs-number>10</span>, <span class=hljs-number>11</span>, <span class=hljs-number>12</span>, <span class=hljs-number>13</span>, <span class=hljs-number>14</span>, <span class=hljs-number>15</span>, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// 6</span>
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// 7</span>
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// 8</span>
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// 9</span>
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// A</span>
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// B</span>
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// C</span>
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// D</span>
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// E</span>
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, <span class=hljs-comment>// F</span>
    ]
};

<span class=hljs-keyword>fn</span> <span class="hljs-title function_">decode_hex_val</span>(val: <span class=hljs-type>u8</span>) <span class=hljs-punctuation>-></span> <span class=hljs-type>Option</span><<span class=hljs-type>u16</span>> {
    <span class=hljs-keyword>let</span> <span class=hljs-variable>n</span> = HEX[val <span class=hljs-keyword>as</span> <span class=hljs-type>usize</span>] <span class=hljs-keyword>as</span> <span class=hljs-type>u16</span>;
    <span class=hljs-keyword>if</span> n == <span class=hljs-number>255</span> {
        <span class=hljs-literal>None</span>
    } <span class=hljs-keyword>else</span> {
        <span class="hljs-title function_ invoke__">Some</span>(n)
    }
}
</code></pre><p>This is what <code>serde_json</code> used to utilize, and it actually worked pretty well (better than <code>std</code>, anyway: <code>std</code> has to be generic over radix, <code>serde_json</code> doesn‚Äôt). This function would then be used like this:<pre><code class=language-rust><span class=hljs-keyword>let</span> <span class=hljs-keyword>mut </span><span class=hljs-variable>n</span> = <span class=hljs-number>0</span>;
<span class=hljs-keyword>for</span> <span class=hljs-variable>_</span> <span class=hljs-keyword>in</span> <span class=hljs-number>0</span>..<span class=hljs-number>4</span> {
    n = (n << <span class=hljs-number>4</span>) + <span class="hljs-title function_ invoke__">decode_hex_val</span>(<span class=hljs-keyword>self</span>.slice[<span class=hljs-keyword>self</span>.index])?;
    <span class=hljs-keyword>self</span>.index += <span class=hljs-number>1</span>;
}
</code></pre><p>That‚Äôs at least 3 <code>shl</code>s and 3 <code>add</code>s, with quite a few <code>mov</code>s, <code>cmp</code>s with 255, and conditional jumps inbetween. We can do better.<p>Let‚Äôs start by removing the <code>?</code> on each iteration. That‚Äôs quite simple. Instead of storing <code>HEX</code> as a <code>[u8; 256]</code> array, we can store it as a <code>[u32; 256]</code> array, mapping <code>__</code> to <code>u32::MAX</code>. No valid digit has the high 16 bits set, so we can easily figure out if some digit was invalid <em>after</em> the loop:<pre><code class=language-rust><span class=hljs-keyword>let</span> <span class=hljs-keyword>mut </span><span class=hljs-variable>n</span> = <span class=hljs-number>0</span>;
<span class=hljs-keyword>for</span> <span class=hljs-variable>_</span> <span class=hljs-keyword>in</span> <span class=hljs-number>0</span>..<span class=hljs-number>4</span> {
    n = (n << <span class=hljs-number>4</span>) + HEX[<span class=hljs-keyword>self</span>.slice[<span class=hljs-keyword>self</span>.index] <span class=hljs-keyword>as</span> <span class=hljs-type>usize</span>];
    <span class=hljs-keyword>self</span>.index += <span class=hljs-number>1</span>;
}
ensure!(n >= <span class=hljs-number>65536</span>, <span class=hljs-string>"Invalid Unicode escape"</span>);
<span class=hljs-keyword>let</span> <span class=hljs-variable>n</span> = n <span class=hljs-keyword>as</span> <span class=hljs-type>u16</span>;
</code></pre><p class=next-group><span class=side-header><span>Punching out</span></span>Saving memory (and thus cache!) by using <code>u16</code> instead of <code>u32</code> looks impossible, because a <code>u16::MAX = 0xFFFF</code> in the leading digit would quickly get shifted to <code>0xFxxx</code> in <code>n</code>, and at that point you can‚Äôt disambiguate between a valid codepoint and an invalid digit.<p>Or is it? Here‚Äôs a trick <a href=https://github.com/yuki0iq/>Yuki</a> invented. We can map <code>__</code> to <code>u16::MAX</code>, but also replace <code>n << 4</code> with <code>n.rotate_left(4)</code> and addition with bitwise OR:<pre><code class=language-rust><span class=hljs-keyword>let</span> <span class=hljs-keyword>mut </span><span class=hljs-variable>n</span> = <span class=hljs-number>0</span>;
<span class=hljs-keyword>for</span> <span class=hljs-variable>_</span> <span class=hljs-keyword>in</span> <span class=hljs-number>0</span>..<span class=hljs-number>4</span> {
    n = n.<span class="hljs-title function_ invoke__">rotate_left</span>(<span class=hljs-number>4</span>) | HEX[<span class=hljs-keyword>self</span>.slice[<span class=hljs-keyword>self</span>.index] <span class=hljs-keyword>as</span> <span class=hljs-type>usize</span>];
    <span class=hljs-keyword>self</span>.index += <span class=hljs-number>1</span>;
}
</code></pre><p>If all hex digits are valid, nothing‚Äôs changed, <code>n</code> is still our codepoint. Rotation is exactly as efficient as shifts on x86, so no issues performance-wise either. But if some hex digit is invalid, it‚Äôs going to ‚Äúpollute‚Äù <code>n</code>, setting it to <code>0xFFFF</code>, and the next iterations <em>will keep yielding <code>0xFFFF</code></em>. Unicode defines <code>U+FFFF</code> as a codepoint that does not signify a character, meaning that it‚Äôs extremely unlikely to be used in realistic data, so we can just branch on <code>n == 0xFFFF</code> afterwards and re-verify if we should emit an error or the JSON genuinely contained a <code>\uFFFF</code>. Isn‚Äôt that neat?<p class=next-group><span class=side-header><span>clueless.jpg</span></span>Just as I was writing this post I <a href=https://github.com/serde-rs/json/pull/1178>realized</a> that this is a classical case of overengineering. The <code>0xFFFF</code> only gets shifted out if we compute the codepoint in 16-bit arithmetic. But we aren‚Äôt in the stone age; we have 32-bit integers! Let‚Äôs store <code>HEX</code> as <code>[i8; 256]</code>, with <code>-1</code> stands for an invalid digit. Then<pre><code class=language-rust><span class=hljs-keyword>let</span> <span class=hljs-keyword>mut </span><span class=hljs-variable>n</span> = <span class=hljs-number>0</span>;
<span class=hljs-keyword>for</span> <span class=hljs-variable>_</span> <span class=hljs-keyword>in</span> <span class=hljs-number>0</span>..<span class=hljs-number>4</span> {
    n = (n << <span class=hljs-number>4</span>) | HEX[<span class=hljs-keyword>self</span>.slice[<span class=hljs-keyword>self</span>.index] <span class=hljs-keyword>as</span> <span class=hljs-type>usize</span>] <span class=hljs-keyword>as</span> <span class=hljs-type>i32</span>;
    <span class=hljs-keyword>self</span>.index += <span class=hljs-number>1</span>;
}
</code></pre><p>‚Ä¶will produce a non-negative number on success and a negative number on failure. The seemingly operational <code>as i32</code> turns out to be a no-op because x86 fuses a memory load and a sign extension into one <code>movsx</code> instruction.<p>What I like about signed numbers is that most processors have a single instruction to branch on sign. Instead of <code>cmp r, imm; je label</code>, we can just do <code>js label</code> in most cases. This does not usually affect performance on modern CPUs, but hey, at least it looks prettier.<p class=next-group><span class=side-header><span>Shifts</span></span>Shifts increase latency. Latency bad. Alisa want no latency.<p>Luckily, this is easy to fix by introducing two tables instead of one: <code>HEX0</code>, which is <code>HEX</code> casted to <code>[i16; 256]</code>, and <code>HEX1</code>, which is <code>HEX</code> casted to <code>[i16; 256]</code> but also left-shifted by <code>4</code>. This allows the loop to be unrolled very clearly and is the final hex-decoding implementation.<pre><code class=language-rust><span class=hljs-keyword>fn</span> <span class="hljs-title function_">decode_four_hex_digits</span>(a: <span class=hljs-type>u8</span>, b: <span class=hljs-type>u8</span>, c: <span class=hljs-type>u8</span>, d: <span class=hljs-type>u8</span>) <span class=hljs-punctuation>-></span> <span class=hljs-type>Option</span><<span class=hljs-type>u16</span>> {
    <span class=hljs-keyword>let</span> <span class=hljs-variable>a</span> = HEX1[a <span class=hljs-keyword>as</span> <span class=hljs-type>usize</span>] <span class=hljs-keyword>as</span> <span class=hljs-type>i32</span>;
    <span class=hljs-keyword>let</span> <span class=hljs-variable>b</span> = HEX0[b <span class=hljs-keyword>as</span> <span class=hljs-type>usize</span>] <span class=hljs-keyword>as</span> <span class=hljs-type>i32</span>;
    <span class=hljs-keyword>let</span> <span class=hljs-variable>c</span> = HEX1[c <span class=hljs-keyword>as</span> <span class=hljs-type>usize</span>] <span class=hljs-keyword>as</span> <span class=hljs-type>i32</span>;
    <span class=hljs-keyword>let</span> <span class=hljs-variable>d</span> = HEX0[d <span class=hljs-keyword>as</span> <span class=hljs-type>usize</span>] <span class=hljs-keyword>as</span> <span class=hljs-type>i32</span>;

    <span class=hljs-keyword>let</span> <span class=hljs-variable>codepoint</span> = ((a | b) << <span class=hljs-number>8</span>) | c | d;

    <span class=hljs-comment>// A single sign bit check.</span>
    <span class=hljs-keyword>if</span> codepoint >= <span class=hljs-number>0</span> {
        <span class="hljs-title function_ invoke__">Some</span>(codepoint <span class=hljs-keyword>as</span> <span class=hljs-type>u16</span>)
    } <span class=hljs-keyword>else</span> {
        <span class=hljs-literal>None</span>
    }
}
</code></pre><p>Overall, this increased the performance of parsing JSON-encoded <em>War and Peace</em> in Russian from 284 MB/s to 344 MB/s, resulting in a 21% improvement.<h2>Transcoding</h2><p class=next-group><span class=side-header><span>Hazards</span></span>After the last optimization, the slowest part of the Unicode string parsing code changed to UTF-8 encoding.<p>This is really funny, because UTF-8 is supposed to be really simple. To give a quick reminder, UTF-8 encodes codepoints in one of the following ways:<ul><li>1 byte: <code>0xxxxxxx</code><li>2 bytes: <code>110xxxxx 10xxxxxx</code><li>3 bytes: <code>1110xxxx 10xxxxxx 10xxxxxx</code><li>4 bytes: <code>11110xxx 10xxxxxx 10xxxxxx 10xxxxxx</code></ul><p>The <code>x</code>s signify the bits of a codepoint; the shortest representation long enough to fit all the bits is used. All codepoints fit in 21 bits.<p>Rust‚Äôs standard library implements UTF-8 encoding by providing, among other functions, <code>char::encode_utf8</code> to store the char encoding to a buffer. There is, however, one minor inconvenience. The signature of the method is<pre><code class=language-rust><span class=hljs-keyword>fn</span> <span class="hljs-title function_">encode_utf8</span>(<span class=hljs-keyword>self</span>, dst: &<span class=hljs-keyword>mut</span> [<span class=hljs-type>u8</span>]) <span class=hljs-punctuation>-></span> &<span class=hljs-keyword>mut</span> <span class=hljs-type>str</span>;
</code></pre><p>‚Ä¶which means that it writes to a buffer that already stores <em>valid</em> <code>u8</code>s. You can‚Äôt just create an <em>uninitialized</em> buffer and put UTF-8 there; you need to initialize (e.g. zero-initialize) it in advance.<p>The assumption here is that the optimizer is smart enough to optimize out zeroization. This <em>would</em> be true in other cases, but UTF-8 is a <em>variable-length</em> encoding. If you zeroed more bytes than <code>encode_utf8</code> would place, zeroization would be wrong to optimize out. So you need to zeroize a variable amount of bytes. But the semantics become too complicated to capture accurately for LLVM here, so it just drops the ball.<hr><p>So <code>serde_json</code> used another approach:<pre><code class=language-rust>scratch.<span class="hljs-title function_ invoke__">extend_from_slice</span>(c.<span class="hljs-title function_ invoke__">encode_utf8</span>(&<span class=hljs-keyword>mut</span> [<span class=hljs-number>0u8</span>; <span class=hljs-number>4</span>]).<span class="hljs-title function_ invoke__">as_bytes</span>());
</code></pre><p><code>[0u8; 4]</code> is a local variable, so zeroing more bytes than necessary shouldn‚Äôt be a problem because aliasing analysis should help with this. Which is kind of true in theory.<p>In practice, something horrendous happens instead. Remember how LLVM drops the ball on variable-length zeroization? Well, it drops the ball on a variable-length copy too. <code>Vec::extend_from_slice</code> needs to copy 1 to 4 bytes from the local buffer to heap, so LLVM invokes glibc‚Äôs <code>memcpy</code> to do that. Wonderful.<hr><p>The best way to avoid calls to <code>memset</code> and <code>memcpy</code> turned out to be generating UTF-8 manually. This is trivial algorithm-wise, but requires unsafety, so I was initially somewhat scared of that, but I had to submit.<p>Together with a few other minor modifications, this further increased performance on <em>War and Peace</em> to 374 MB/s (+9%).<h2>Final results</h2><p class=next-group><span class=side-header><span>ü•∫</span></span>All in all, my work improved <code>serde_json</code> performance on various string-heavy JSON benchmarks by 10%, 23%, and 32%. A lot of JSON data contains many strings, so I believe that this will benefit the ecosystem in the long run.</div></section><footer><div class=viewport-container><h2>Made with my own bare hands (why.)</h2></div></footer>